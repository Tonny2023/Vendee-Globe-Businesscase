{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "# making a table out of our data stored bij the streaming analytics Job in the blobcontainer\n",
    "blob_account_name = \"storageb2group3\"\n",
    "blob_container_name = \"streamingoutputcontainer\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
    "blob_sas_token = token_library.getConnectionString(\"AzureBlobStorage1\")\n",
    "\n",
    "spark.conf.set(\n",
    "    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "    blob_sas_token)\n",
    "df = spark.read.load('wasbs://streamingoutputcontainer@storageb2group3.blob.core.windows.net/sailingdataoutput/*', format='csv'\n",
    "## If header exists uncomment line below\n",
    ", header=True\n",
    ")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# getting only the latest 10 rows in, one for each boat\n",
    "w = W.partitionBy('boat').orderBy(F.desc('EventEnqueuedUtcTime'))\n",
    "df = (df\n",
    "    .withColumn('latest', F.first('EventEnqueuedUtcTime').over(w))\n",
    "    .filter('EventEnqueuedUtcTime = latest')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import sqrt, sin, cos, udf\n",
    "import numpy as np\n",
    "\n",
    "# calculate the distance of each boat from the first point to the latest\n",
    "def haversine(lat2, lon2):\n",
    "    lon1 = np.radians(-9.41)\n",
    "    lat1 = np.radians(38.69)\n",
    "    lon2 = np.radians(float(lon2))\n",
    "    lat2 = np.radians(float(lat2)) \n",
    "    newlon = lon2 - lon1\n",
    "    newlat = lat2 - lat1\n",
    "    haver_formula = (\n",
    "        np.sin(newlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon / 2.0) ** 2 )\n",
    "    haver_formula = float(haver_formula)\n",
    "    dist = 2 * np.arcsin(np.sqrt(haver_formula))\n",
    "    \n",
    "    return float(6373.0) * float(dist)\n",
    "\n",
    "dist_udf = F.udf(haversine,FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add column with distance\n",
    "df_new = df.withColumn(\"distance\", dist_udf(df['latitude'], df['longitude']))\n",
    "\n",
    "# make new table with boat, distance, header df.first() drop\n",
    "#df_final = df.select(df.columns[0:4]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Use same name for file every time when overwriting\n",
    "def write_csv_with_specific_file_name(sc, df, path, filename):\n",
    "    file_format = 'csv'\n",
    "    df.repartition(1).write.option(\"header\", \"true\").format(file_format).mode(\"overwrite\").save(path)\n",
    "    try:\n",
    "        sc_uri = sc._gateway.jvm.java.net.URI\n",
    "        sc_path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "        file_system = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "        configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "        fs = file_system.get(sc_uri(\"wasbs://streamingoutputcontainer@storageb2group3.blob.core.windows.net/latest_output_table\"), configuration())\n",
    "        src_path = None\n",
    "        status = fs.listStatus(sc_path(path))\n",
    "        for fileStatus in status:\n",
    "            temp = fileStatus.getPath().toString()\n",
    "            if \"part\" in temp:\n",
    "                src_path = sc_path(temp)\n",
    "        dest_path = sc_path(path + filename)\n",
    "        if fs.exists(src_path) and fs.isFile(src_path):\n",
    "            fs.rename(src_path, dest_path)\n",
    "            fs.delete(src_path, True)\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Error renaming the part file to {}:\".format(filename, e))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    write_csv_with_specific_file_name(spark.sparkContext, df_new, \"wasbs://streamingoutputcontainer@storageb2group3.blob.core.windows.net/latest_output_table\", \"/sailingdata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
